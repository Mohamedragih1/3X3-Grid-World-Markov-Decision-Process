{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamedragih1/3X3-Grid-World-Markov-Decision-Process/blob/main/Markov_Decision_Process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESMWQEjyYRQa"
      },
      "source": [
        "# Assignment 4\n",
        "\n",
        "# Markov Decision Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7FcYnsEYRQd"
      },
      "source": [
        "Defines parameters and initializes variables for a Markov Decision Process (MDP) simulation:\n",
        "\n",
        "- `GRID_SIZE = 3`: Sets the size of the grid for the MDP.\n",
        "- `R_X` and `R_Y`: Placeholder variables for the current position in the grid (not used in the provided code snippet).\n",
        "- `action_symbols`: Defines symbols for actions ('↑' for up, '↓' for down, '→' for right, '←' for left).\n",
        "- `actions`: Alias for `action_symbols`.\n",
        "- `gamma = 0.99`: Sets the discount factor for future rewards.\n",
        "- `rewards`: Defines the reward matrix for each state-action pair in the MDP.\n",
        "- `transition_probs`: Specifies the transition probabilities for each action in the MDP.\n",
        "- `value_function`: Initializes the value function matrix with zeros.\n",
        "\n",
        "The `rewards` matrix holds the immediate rewards for transitioning from one state to another based on the action taken. The `transition_probs` dictionary maps each action to a list of probabilities corresponding to the likelihood of transitioning to each possible next state.\n",
        "\n",
        "The value function `value_function` is initialized as a square matrix of zeros with dimensions `GRID_SIZE` by `GRID_SIZE`, representing the estimated value of each state in the MDP.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-eP2I9qYRQe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# MDP parameters\n",
        "GRID_SIZE = 3  # Size of the grid\n",
        "R_X = 0  # X of the reward state\n",
        "R_Y = 0  # Y of the reward state\n",
        "\n",
        "# Define action symbols\n",
        "action_symbols = ['↑', '↓', '→', '←']  # Symbols for the four possible actions\n",
        "\n",
        "actions = action_symbols\n",
        "gamma = 0.99\n",
        "\n",
        "# Define the rewards matrix\n",
        "rewards = np.array([[0, -1, 10],\n",
        "                    [-1, -1, -1],\n",
        "                    [-1, -1, -1]])\n",
        "\n",
        "# rewards = np.array([[0, -1, -1, 10],\n",
        "#                     [-1, -1, -1, -1],\n",
        "#                     [-1, -1, -1, -1],\n",
        "#                     [-1, -1, -1, -1]])\n",
        "\n",
        "# Transition probabilities for each action\n",
        "transition_probs = {'↑': [0.8, 0.0, 0.1, 0.1],\n",
        "                    '↓': [0, 0.8, 0.1, 0.1],\n",
        "                    '→': [0.1, 0.1, 0.8, 0],\n",
        "                    '←': [0.1, 0.1, 0, 0.8]}\n",
        "\n",
        "# Initialize value function for each state in the grid\n",
        "value_function = np.zeros((GRID_SIZE, GRID_SIZE))\n",
        "policy = np.zeros((GRID_SIZE, GRID_SIZE), dtype=str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVpJskxCYRQf"
      },
      "source": [
        "The `get_new_position` function takes as input the current position `(i, j)` in a grid and an action (`'↑'`, `'↓'`, `'→'`, `'←'`) representing a movement direction. It then calculates the new position after taking that action within the constraints of the grid.\n",
        "\n",
        "- If the action is `'↑'`, it checks if moving up would result in a position above the grid boundary (`max(i - 1, 0)`) and returns the new position `(i - 1, j)` if valid.\n",
        "- If the action is `'↓'`, it checks if moving down would result in a position below the grid boundary (`min(i + 1, GRID_SIZE - 1)`) and returns the new position `(i + 1, j)` if valid.\n",
        "- If the action is `'→'`, it checks if moving right would result in a position beyond the grid boundary (`min(j + 1, GRID_SIZE - 1)`) and returns the new position `(i, j + 1)` if valid.\n",
        "- If the action is `'←'`, it checks if moving left would result in a position before the grid boundary (`max(j - 1, 0)`) and returns the new position `(i, j - 1)` if valid.\n",
        "\n",
        "This function ensures that the new position is within the bounds of the grid (`GRID_SIZE`) based on the specified action, preventing movements outside the grid boundaries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2pp6cUQYRQf"
      },
      "outputs": [],
      "source": [
        "def get_new_position(i, j, action):\n",
        "    if action == '↑':\n",
        "        return max(i - 1, 0), j\n",
        "    elif action == '↓':\n",
        "        return min(i + 1, GRID_SIZE - 1), j\n",
        "    elif action == '→':\n",
        "        return i, min(j + 1, GRID_SIZE - 1)\n",
        "    elif action == '←':\n",
        "        return i, max(j - 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jchNjbymYRQf"
      },
      "source": [
        "The `valid_action` function takes as input the current position `(i, j)` in a grid and an action (`'↑'`, `'↓'`, `'→'`, `'←'`) representing a movement direction. It then calculates the new position after taking that action within the constraints of the grid.\n",
        "\n",
        "The function returns `True` if valid move and `False` if not valid.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mznKm6XlYRQg"
      },
      "outputs": [],
      "source": [
        "def valid_action(i, j, action):\n",
        "    if action == '↑':\n",
        "        if i <= 0:\n",
        "            return False\n",
        "    elif action == '↓':\n",
        "        if i >= GRID_SIZE - 1:\n",
        "            return False\n",
        "    elif action == '→':\n",
        "        if j >= GRID_SIZE - 1:\n",
        "            return False\n",
        "\n",
        "    elif action == '←':\n",
        "        if j <= 0:\n",
        "            return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr3dEN2gYRQg"
      },
      "source": [
        "# Value Iteration\n",
        "\n",
        "It uses a convergence threshold (`theta`) and tracks the number of iterations (`iteration_count`) until convergence.\n",
        "\n",
        "- `theta`: Convergence threshold to determine when to stop iterating.\n",
        "- `iteration_count`: Tracks the number of iterations for convergence.\n",
        "\n",
        "The algorithm iterates through the MDP grid, skips terminal states, and updates non-terminal states' values using the Bellman optimality equation. It stops iterating when the maximum change in the value function (`delta`) falls below the threshold (`theta`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWCPtuo8YRQg"
      },
      "outputs": [],
      "source": [
        "def value_iteration():\n",
        "    theta = 0.001\n",
        "    iteration_count = 0\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for i in range(GRID_SIZE):\n",
        "            for j in range(GRID_SIZE):\n",
        "                if rewards[i][j] == 10 or rewards[i][j] == rewards[R_X][R_Y]:  # Skip terminal states\n",
        "                    continue\n",
        "\n",
        "                v = value_function[i][j]\n",
        "                new_v = float('-inf')\n",
        "                best_action = None\n",
        "                for action in actions:\n",
        "                    if not valid_action(i, j, action):\n",
        "                        continue\n",
        "\n",
        "                    action_probabilities = transition_probs[action]\n",
        "                    action_value = rewards[i][j]\n",
        "\n",
        "                    for k, prob in enumerate(action_probabilities):\n",
        "                        new_i, new_j = get_new_position(i, j, action_symbols[k])\n",
        "                        action_value += prob * gamma * value_function[new_i][new_j]\n",
        "\n",
        "                    if action_value > new_v:\n",
        "                        new_v = action_value\n",
        "                        best_action = action\n",
        "\n",
        "                value_function[i][j] = new_v\n",
        "                policy[i][j] = best_action\n",
        "                delta = max(delta, abs(v - value_function[i][j]))\n",
        "        iteration_count += 1\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return iteration_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEm8_HY3YRQg"
      },
      "source": [
        "# Run Value Iteration Function\n",
        "\n",
        "Runs the Value Iteration algorithm.\n",
        "\n",
        "- Returns: The number of iterations required for convergence (`iteration_count`).\n",
        "\n",
        "Calls the `value_iteration()` function to compute the optimal value function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iMygYYdYRQh"
      },
      "outputs": [],
      "source": [
        "def run_value_iteration(reward_value):\n",
        "\n",
        "    global rewards\n",
        "    rewards[R_X][R_Y] = reward_value\n",
        "    return value_iteration()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKnGoG-fYRQh"
      },
      "source": [
        "## Main call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voIdbD8GYRQh",
        "outputId": "1407b0da-572b-49ef-8b8e-2f3e3920ceca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Optimal Value Function for r = 100 (Converged in 13 iterations):\n",
            "[[100.          97.20351875  10.        ]\n",
            " [ 97.20351875  94.7512665   88.20399982]\n",
            " [ 94.48181286  92.35290246  89.76213039]]\n",
            "\n",
            "Optimal Policy for r = 100\n",
            "[['r' '←' 'T']\n",
            " ['↑' '←' '↓']\n",
            " ['↑' '←' '←']]\n",
            "\n",
            "Optimal Value Function for r = 3 (Converged in 41 iterations):\n",
            "[[ 3.          8.46194904 10.        ]\n",
            " [ 5.38366785  7.11322545  8.46194227]\n",
            " [ 4.57497057  5.79414801  6.96501766]]\n",
            "\n",
            "Optimal Policy for r = 3\n",
            "[['r' '→' 'T']\n",
            " ['→' '→' '↑']\n",
            " ['→' '→' '↑']]\n",
            "\n",
            "Optimal Value Function for r = 0 (Converged in 3 iterations):\n",
            "[[ 0.          8.46193956 10.        ]\n",
            " [ 5.08334043  7.1132055   8.46193936]\n",
            " [ 4.54187018  5.79411233  6.96500905]]\n",
            "\n",
            "Optimal Policy for r = 0\n",
            "[['r' '→' 'T']\n",
            " ['→' '→' '↑']\n",
            " ['→' '→' '↑']]\n",
            "\n",
            "Optimal Value Function for r = -3 (Converged in 3 iterations):\n",
            "[[-3.          8.46193928 10.        ]\n",
            " [ 4.78307131  7.11320493  8.46193928]\n",
            " [ 4.50887324  5.79411129  6.9650088 ]]\n",
            "\n",
            "Optimal Policy for r = -3\n",
            "[['r' '→' 'T']\n",
            " ['→' '→' '↑']\n",
            " ['→' '→' '↑']]\n"
          ]
        }
      ],
      "source": [
        "r_values = [100, 3, 0, -3]\n",
        "for r in r_values:\n",
        "    value_function[R_X, R_Y] = r\n",
        "    value_function[0, -1] = 10\n",
        "\n",
        "\n",
        "    num_iterations = run_value_iteration(r)\n",
        "\n",
        "    print(f\"\\nOptimal Value Function for r = {r} (Converged in {num_iterations} iterations):\")\n",
        "    print(value_function)\n",
        "    policy[R_X, R_Y] = 'r'\n",
        "    policy[0,-1] = 'T'\n",
        "    print(f\"\\nOptimal Policy for r = {r}\")\n",
        "    print(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S64cfif2YRQh"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "When r is bigger then 10 the algorithm will go toward it otherwise go to the 10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clW1qPoeYRQi"
      },
      "source": [
        "# BONUS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3YlFQ6WYRQi"
      },
      "source": [
        "Additional to `value_function` we will add `policy` which will store the optimal policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvYVXnwPYRQi"
      },
      "outputs": [],
      "source": [
        "# Initialize a random policy\n",
        "policy = np.random.choice(action_symbols, size=(GRID_SIZE, GRID_SIZE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zcXOm4_YRQi"
      },
      "source": [
        "# Policy Iteration Function\n",
        "\n",
        "Implements the Policy Iteration algorithm to compute the optimal policy and value function.\n",
        "\n",
        "- `iteration_count`: Counter for iterations.\n",
        "\n",
        "The algorithm consists of two main steps:\n",
        "\n",
        "1. **Policy Evaluation**:\n",
        "   - Iteratively evaluates the current policy until convergence using a specified convergence threshold (`theta`).\n",
        "   - Updates the value function based on the Bellman expectation equation.\n",
        "\n",
        "2. **Policy Improvement**:\n",
        "   - Determines the best action for each state by considering possible actions and their expected rewards.\n",
        "   - Updates the policy if a better action is found for any state.\n",
        "   - Checks for policy stability, where no further changes occur in the policy.\n",
        "\n",
        "The algorithm continues iterating between policy evaluation and improvement until the policy becomes stable.\n",
        "\n",
        "The function returns the number of iterations (`iteration_count`) required for convergence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1529kJuYRQi"
      },
      "outputs": [],
      "source": [
        "def policy_iteration():\n",
        "    theta = 0.01\n",
        "    iteration_count = 0  # Counter for iterations\n",
        "    while True:\n",
        "        iteration_count += 1\n",
        "\n",
        "        # Policy Evaluation\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for i in range(GRID_SIZE):\n",
        "                for j in range(GRID_SIZE):\n",
        "                    if rewards[i][j] == 10 or rewards[i][j] == rewards[R_X][R_Y]:  # Skip terminal states\n",
        "                        continue\n",
        "\n",
        "                    v = value_function[i][j]\n",
        "                    action = policy[i][j]\n",
        "                    action_probabilities = transition_probs[action]\n",
        "                    action_value = rewards[i][j]\n",
        "                    for k, prob in enumerate(action_probabilities):\n",
        "                        new_i, new_j = get_new_position(i, j, action_symbols[k])\n",
        "                        action_value += prob * gamma * value_function[new_i][new_j]\n",
        "                    value_function[i][j] = action_value\n",
        "                    delta = max(delta, abs(v - value_function[i][j]))\n",
        "            if delta < theta:  # Convergence threshold\n",
        "                break\n",
        "\n",
        "        # Policy Improvement\n",
        "        policy_stable = True\n",
        "        for i in range(GRID_SIZE):\n",
        "            for j in range(GRID_SIZE):\n",
        "                if rewards[i][j] == 10 or rewards[i][j] == rewards[R_X][R_Y]:  # Skip terminal states\n",
        "                    continue\n",
        "\n",
        "                old_action = policy[i][j]\n",
        "                best_action = None\n",
        "                best_value = float('-inf')\n",
        "\n",
        "                for action in actions:\n",
        "                    if not valid_action(i, j, action):\n",
        "                        continue\n",
        "\n",
        "                    action_probabilities = transition_probs[action]\n",
        "                    action_value = rewards[i][j]\n",
        "\n",
        "                    for k, prob in enumerate(action_probabilities):\n",
        "                        new_i, new_j = get_new_position(i, j, action_symbols[k])\n",
        "                        action_value += prob * gamma * value_function[new_i][new_j]\n",
        "\n",
        "                    if action_value > best_value:\n",
        "                        best_value = action_value\n",
        "                        best_action = action\n",
        "                policy[i][j] = best_action\n",
        "\n",
        "                # Check for policy stability based on action change\n",
        "                if old_action != best_action:\n",
        "                    policy_stable = False\n",
        "\n",
        "        # Break if policy remains stable after full iteration\n",
        "        if policy_stable:\n",
        "            break\n",
        "\n",
        "    return iteration_count  # Return the number of iterations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iVg5pkXYRQi"
      },
      "source": [
        "# Run Policy Iteration Function\n",
        "\n",
        "The `run_policy_iteration(reward_value)` function runs the Policy Iteration algorithm to compute the optimal policy.\n",
        "\n",
        "- `reward_value`: The reward value to set for the specified state.\n",
        "\n",
        "Calls the `policy_iteration()` function to compute the optimal policy and value function.\n",
        "\n",
        "The function returns the number of iterations required for convergence in the Policy Iteration algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcSfd86YYRQi"
      },
      "outputs": [],
      "source": [
        "def run_policy_iteration(reward_value):\n",
        "    global rewards\n",
        "    rewards[R_X][R_Y] = reward_value\n",
        "    return policy_iteration()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sDFul1rYRQi"
      },
      "source": [
        "## Main call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGLlRgJDYRQj",
        "outputId": "9ab8e2c9-3f28-4acf-f3f5-c5038ee4f0a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Optimal Values for r = 100 (Stabilized in 5 iterations):\n",
            "[[100.          97.20352503  10.        ]\n",
            " [ 97.20352503  94.7512797   88.20192869]\n",
            " [ 94.48183143  92.35292609  89.76179727]]\n",
            "\n",
            "Optimal Policy for r = 100\n",
            "[['r' '←' 'T']\n",
            " ['↑' '←' '↓']\n",
            " ['↑' '←' '←']]\n",
            "\n",
            "Optimal Values for r = 3 (Stabilized in 4 iterations):\n",
            "[[ 3.          8.46182121 10.        ]\n",
            " [ 2.30865806  7.11295718  8.46190306]\n",
            " [ 4.23526615  5.79366842  6.9649018 ]]\n",
            "\n",
            "Optimal Policy for r = 3\n",
            "[['r' '→' 'T']\n",
            " ['↑' '→' '↑']\n",
            " ['→' '→' '↑']]\n",
            "\n",
            "Optimal Values for r = 0 (Stabilized in 2 iterations):\n",
            "[[ 0.          8.46193924 10.        ]\n",
            " [ 5.08260177  7.11320484  8.46193926]\n",
            " [ 4.54105705  5.79411115  6.96500876]]\n",
            "\n",
            "Optimal Policy for r = 0\n",
            "[['r' '→' 'T']\n",
            " ['→' '→' '↑']\n",
            " ['→' '→' '↑']]\n",
            "\n",
            "Optimal Values for r = -3 (Stabilized in 1 iterations):\n",
            "[[-3.          8.46193927 10.        ]\n",
            " [ 4.78337961  7.1132049   8.46193927]\n",
            " [ 4.50921201  5.79411125  6.96500879]]\n",
            "\n",
            "Optimal Policy for r = -3\n",
            "[['r' '→' 'T']\n",
            " ['→' '→' '↑']\n",
            " ['→' '→' '↑']]\n"
          ]
        }
      ],
      "source": [
        "r_values = [100, 3, 0, -3]\n",
        "for r in r_values:\n",
        "    value_function[R_X, R_Y] = r\n",
        "    value_function[0, -1] = 10\n",
        "\n",
        "    num_iterations = run_policy_iteration(r)\n",
        "\n",
        "    optimal_policy = np.where(rewards == 10, 'T', policy)\n",
        "\n",
        "    optimal_policy[R_X][R_Y] = 'r'\n",
        "    print(f\"\\nOptimal Values for r = {r} (Stabilized in {num_iterations} iterations):\")\n",
        "    print(value_function)\n",
        "\n",
        "    policy[R_X, R_Y] = 'r'\n",
        "    policy[0,-1] = 'T'\n",
        "    print(f\"\\nOptimal Policy for r = {r}\")\n",
        "    print(optimal_policy)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}